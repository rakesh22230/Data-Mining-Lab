{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakesh22230/Data-Mining-Lab/blob/main/Bangla_Fake_News_Training_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcWF54j2lhgB",
        "outputId": "cd16f6b2-41a3-4c4d-fb00-bf68131cbaa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing packages one by one...\n",
            "\n",
            "[1/4] Installing scikit-learn...\n",
            "[2/4] Installing pandas...\n",
            "[3/4] Installing numpy...\n",
            "[4/4] Installing scipy...\n",
            "\n",
            "‚úÖ All packages installed!\n",
            "üìã Run the next cell to test imports\n"
          ]
        }
      ],
      "source": [
        "# Install one by one\n",
        "import sys\n",
        "\n",
        "print(\"Installing packages one by one...\")\n",
        "\n",
        "print(\"\\n[1/4] Installing scikit-learn...\")\n",
        "!{sys.executable} -m pip install -q scikit-learn\n",
        "\n",
        "print(\"[2/4] Installing pandas...\")\n",
        "!{sys.executable} -m pip install -q pandas\n",
        "\n",
        "print(\"[3/4] Installing numpy...\")\n",
        "!{sys.executable} -m pip install -q numpy\n",
        "\n",
        "print(\"[4/4] Installing scipy...\")\n",
        "!{sys.executable} -m pip install -q scipy\n",
        "\n",
        "print(\"\\n‚úÖ All packages installed!\")\n",
        "print(\"üìã Run the next cell to test imports\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test imports\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"   scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"   pandas: {pd.__version__}\")\n",
        "print(f\"   numpy: {np.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZOeIbu3mGUk",
        "outputId": "d4e7167c-b45d-4d7c-dad5-1183530a1fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All imports successful!\n",
            "   scikit-learn: 1.6.1\n",
            "   pandas: 2.2.2\n",
            "   numpy: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üìÅ CHECKING DATASET FILES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define the base path for your files in Google Drive\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "\n",
        "# Check for required files\n",
        "required_files = ['train_cleaned.csv', 'val_cleaned.csv', 'test_cleaned.csv']\n",
        "missing_files = []\n",
        "\n",
        "print(f\"\\nüìÇ Looking for files in: {base_path}...\")\n",
        "for file_name in required_files:\n",
        "    file_path = os.path.join(base_path, file_name)\n",
        "    if os.path.exists(file_path):\n",
        "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
        "        print(f\"   ‚úÖ {file_name} - {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå {file_name} - NOT FOUND\")\n",
        "        missing_files.append(file_name)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"\\n‚ùå ERROR: Missing {len(missing_files)} file(s) in {base_path}:\")\n",
        "    for f in missing_files:\n",
        "        print(f\"   - {f}\")\n",
        "    print(\"\\nüí° Instructions:\")\n",
        "    print(f\"   Ensure these files are present in your Google Drive folder: {base_path}\")\n",
        "    print(\"   If the path is incorrect, please let me know the correct path.\")\n",
        "    print(\"   After confirming files are there, re-run this cell.\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ ALL FILES PRESENT!\")\n",
        "    print(\"üöÄ You can now proceed to CELL 3!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIa4EZT4mLO2",
        "outputId": "8388f5ca-928a-4558-f1dd-8ef8d1b3327c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üìÅ CHECKING DATASET FILES\n",
            "================================================================================\n",
            "\n",
            "üìÇ Looking for files in: /content/drive/MyDrive/Colab Notebooks/...\n",
            "   ‚úÖ train_cleaned.csv - 199.72 MB\n",
            "   ‚úÖ val_cleaned.csv - 42.69 MB\n",
            "   ‚úÖ test_cleaned.csv - 42.81 MB\n",
            "================================================================================\n",
            "\n",
            "‚úÖ ALL FILES PRESENT!\n",
            "üöÄ You can now proceed to CELL 3!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os # Import os module to join paths\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä LOADING DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define the base path for your files (same as in the file checking cell)\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/'\n",
        "\n",
        "# Load datasets using the full path\n",
        "train_df = pd.read_csv(os.path.join(base_path, 'train_cleaned.csv'), encoding='utf-8')\n",
        "val_df = pd.read_csv(os.path.join(base_path, 'val_cleaned.csv'), encoding='utf-8')\n",
        "test_df = pd.read_csv(os.path.join(base_path, 'test_cleaned.csv'), encoding='utf-8')\n",
        "\n",
        "print(f\"\\n‚úÖ Training set: {train_df.shape[0]:,} samples √ó {train_df.shape[1]} columns\")\n",
        "print(f\"‚úÖ Validation set: {val_df.shape[0]:,} samples √ó {val_df.shape[1]} columns\")\n",
        "print(f\"‚úÖ Test set: {test_df.shape[0]:,} samples √ó {test_df.shape[1]} columns\")\n",
        "\n",
        "print(f\"\\nüìã Columns: {train_df.columns.tolist()}\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nüì∞ Sample Data (first 2 rows):\")\n",
        "print(train_df.head(2))\n",
        "\n",
        "# Label distribution\n",
        "print(f\"\\nüìä Original Label Distribution:\")\n",
        "label_counts = train_df['Label'].value_counts().sort_index()\n",
        "for label, count in label_counts.items():\n",
        "    print(f\"   Label {label}: {count:,} samples ({count/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n‚úÖ Data loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUmtAo1RmNje",
        "outputId": "2ae692cd-8049-46e8-ca12-7ebc43014b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üìä LOADING DATASET\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Training set: 42,380 samples √ó 3 columns\n",
            "‚úÖ Validation set: 9,082 samples √ó 3 columns\n",
            "‚úÖ Test set: 9,082 samples √ó 3 columns\n",
            "\n",
            "üìã Columns: ['Headline', 'Content', 'Label']\n",
            "\n",
            "üì∞ Sample Data (first 2 rows):\n",
            "                                Headline  \\\n",
            "0          ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶´‡ßã‡¶∞‡¶£‡ßá ‡¶®‡¶ø‡¶π‡¶§ ‡ßß   \n",
            "1  ‡ßß‡ß´ ‡¶ò‡¶£‡ßç‡¶ü‡¶æ ‡¶™‡¶∞ ‡¶Æ‡ßü‡¶Æ‡¶®‡¶∏‡¶ø‡¶Ç‡¶π‡ßá‡¶∞ ‡¶™‡¶•‡ßá ‡¶ü‡ßç‡¶∞‡ßá‡¶® ‡¶ö‡¶æ‡¶≤‡ßÅ   \n",
            "\n",
            "                                             Content  Label  \n",
            "0  ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞...      3  \n",
            "1  ‡¶Æ‡ßü‡¶Æ‡¶®‡¶ø‡¶∏‡¶Ç‡¶π ‡¶∏‡ßç‡¶ü‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶∏‡ßÅ‡¶™‡¶æ‡¶∞‡¶ø‡¶®‡¶ü‡ßá‡¶®‡¶°‡ßá‡¶®‡ßç‡¶ü ‡¶ú‡¶π‡¶ø‡¶∞‡ßÅ‡¶≤ ‡¶π‡¶ï ‡¶ú‡¶æ...      2  \n",
            "\n",
            "üìä Original Label Distribution:\n",
            "   Label 0: 1,524 samples (3.6%)\n",
            "   Label 1: 2,112 samples (5.0%)\n",
            "   Label 2: 4,629 samples (10.9%)\n",
            "   Label 3: 34,115 samples (80.5%)\n",
            "\n",
            "‚úÖ Data loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîß DEFINING PREPROCESSING FUNCTIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Label mapping function\n",
        "def map_labels(label):\n",
        "    \"\"\"\n",
        "    Binary classification:\n",
        "    - Label 1 (Authentic) ‚Üí 1 (Real News)\n",
        "    - Labels 0, 2, 3 (Fake, Satire, Clickbait) ‚Üí 0 (Fake News)\n",
        "    \"\"\"\n",
        "    return 1 if label == 1 else 0\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_bangla_text(text):\n",
        "    \"\"\"\n",
        "    Enhanced Bangla text cleaning:\n",
        "    - Keeps Bengali numbers (‡ß¶-‡ßØ) for dates, scores\n",
        "    - Keeps Bengali punctuation (‡•§ ?)\n",
        "    - Removes only English letters\n",
        "    - Preserves maximum context\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove English letters only\n",
        "    text = re.sub(r'[a-zA-Z]', ' ', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Keep Bengali chars + numbers + punctuation\n",
        "    text = re.sub(r'[^\\u0980-\\u09FF\\s‡ß¶-‡ßØ\\u0964\\u0965]', '', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Minimal stopwords list\n",
        "bangla_stopwords = ['‡¶è‡¶¨‡¶Ç', '‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ', '‡¶Ø‡ßá', '‡¶è‡¶á']\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"Remove only the most common stopwords\"\"\"\n",
        "    words = text.split()\n",
        "    filtered = [word for word in words if word not in bangla_stopwords]\n",
        "    return ' '.join(filtered)\n",
        "\n",
        "# Statistical feature extraction\n",
        "def extract_statistical_features(texts):\n",
        "    \"\"\"\n",
        "    Extract statistical features that help identify fake news:\n",
        "    - Word count, character count, average word length\n",
        "    - Punctuation patterns, question marks, exclamations\n",
        "    - Long and short words, numbers\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    for text in texts:\n",
        "        words = text.split()\n",
        "        features.append([\n",
        "            len(words),                              # Word count\n",
        "            len(text),                               # Character count\n",
        "            len(words) / (len(text) + 1),           # Average word length\n",
        "            text.count('‡•§'),                         # Bengali punctuation\n",
        "            text.count('?'),                         # Question marks\n",
        "            text.count('!'),                         # Exclamations\n",
        "            len([w for w in words if len(w) > 10]), # Long words (>10 chars)\n",
        "            len([w for w in words if len(w) < 3]),  # Short words (<3 chars)\n",
        "            text.count(' ‡ß¶') + text.count(' ‡ßß') + text.count(' ‡ß®'),  # Number mentions\n",
        "        ])\n",
        "    return np.array(features)\n",
        "\n",
        "print(\"‚úÖ All preprocessing functions defined!\")\n",
        "print(\"\\nüìã Functions:\")\n",
        "print(\"   ‚úÖ map_labels() - Convert to binary (0=Fake, 1=Real)\")\n",
        "print(\"   ‚úÖ clean_bangla_text() - Clean and normalize text\")\n",
        "print(\"   ‚úÖ remove_stopwords() - Remove common words\")\n",
        "print(\"   ‚úÖ extract_statistical_features() - Extract 9 statistical features\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ii9gLq7mOU_",
        "outputId": "699b156e-aa28-498e-9e5d-da971dd13a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üîß DEFINING PREPROCESSING FUNCTIONS\n",
            "================================================================================\n",
            "‚úÖ All preprocessing functions defined!\n",
            "\n",
            "üìã Functions:\n",
            "   ‚úÖ map_labels() - Convert to binary (0=Fake, 1=Real)\n",
            "   ‚úÖ clean_bangla_text() - Clean and normalize text\n",
            "   ‚úÖ remove_stopwords() - Remove common words\n",
            "   ‚úÖ extract_statistical_features() - Extract 9 statistical features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"üîÑ PREPROCESSING DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Step 1: Map labels to binary\n",
        "print(\"\\n[1/3] Mapping labels to binary...\")\n",
        "train_df['Label_Binary'] = train_df['Label'].apply(map_labels)\n",
        "val_df['Label_Binary'] = val_df['Label'].apply(map_labels)\n",
        "test_df['Label_Binary'] = test_df['Label'].apply(map_labels)\n",
        "\n",
        "fake_count = (train_df['Label_Binary'] == 0).sum()\n",
        "real_count = (train_df['Label_Binary'] == 1).sum()\n",
        "print(f\"   Fake News (0): {fake_count:,} samples ({fake_count/len(train_df)*100:.1f}%)\")\n",
        "print(f\"   Real News (1): {real_count:,} samples ({real_count/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "# Step 2: Combine Headline and Content\n",
        "print(\"\\n[2/3] Combining Headline + Content...\")\n",
        "train_df['full_text'] = train_df['Headline'].fillna('') + ' ' + train_df['Content'].fillna('')\n",
        "val_df['full_text'] = val_df['Headline'].fillna('') + ' ' + val_df['Content'].fillna('')\n",
        "test_df['full_text'] = test_df['Headline'].fillna('') + ' ' + test_df['Content'].fillna('')\n",
        "\n",
        "# Step 3: Clean and remove stopwords\n",
        "print(\"\\n[3/3] Cleaning text and removing stopwords...\")\n",
        "train_df['cleaned_text'] = train_df['full_text'].apply(clean_bangla_text).apply(remove_stopwords)\n",
        "val_df['cleaned_text'] = val_df['full_text'].apply(clean_bangla_text).apply(remove_stopwords)\n",
        "test_df['cleaned_text'] = test_df['full_text'].apply(clean_bangla_text).apply(remove_stopwords)\n",
        "\n",
        "# Remove very short texts\n",
        "initial_train = len(train_df)\n",
        "initial_test = len(test_df)\n",
        "train_df = train_df[train_df['cleaned_text'].str.len() > 10]\n",
        "test_df = test_df[test_df['cleaned_text'].str.len() > 10]\n",
        "\n",
        "print(f\"   Removed {initial_train - len(train_df)} short training samples\")\n",
        "print(f\"   Removed {initial_test - len(test_df)} short test samples\")\n",
        "\n",
        "print(f\"\\n‚úÖ Preprocessing complete!\")\n",
        "print(f\"   Final training samples: {len(train_df):,}\")\n",
        "print(f\"   Final test samples: {len(test_df):,}\")\n",
        "\n",
        "# Show sample cleaned text\n",
        "print(f\"\\nüìù Sample Cleaned Text:\")\n",
        "sample_original = train_df['full_text'].iloc[0][:100]\n",
        "sample_cleaned = train_df['cleaned_text'].iloc[0][:100]\n",
        "print(f\"   Original: {sample_original}...\")\n",
        "print(f\"   Cleaned:  {sample_cleaned}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B5PYP8ymREA",
        "outputId": "fb58c6a7-f2db-45d0-a2ef-8b7ae4713735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üîÑ PREPROCESSING DATA\n",
            "================================================================================\n",
            "\n",
            "[1/3] Mapping labels to binary...\n",
            "   Fake News (0): 40,268 samples (95.0%)\n",
            "   Real News (1): 2,112 samples (5.0%)\n",
            "\n",
            "[2/3] Combining Headline + Content...\n",
            "\n",
            "[3/3] Cleaning text and removing stopwords...\n",
            "   Removed 5 short training samples\n",
            "   Removed 2 short test samples\n",
            "\n",
            "‚úÖ Preprocessing complete!\n",
            "   Final training samples: 42,375\n",
            "   Final test samples: 9,080\n",
            "\n",
            "üìù Sample Cleaned Text:\n",
            "   Original: ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶´‡ßã‡¶∞‡¶£‡ßá ‡¶®‡¶ø‡¶π‡¶§ ‡ßß ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶´‡ßã‡¶∞‡¶£‡ßá ‡¶Ü‡¶ü ‡¶¨‡¶õ‡¶∞‡ßá...\n",
            "   Cleaned:  ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶´‡ßã‡¶∞‡¶£‡ßá ‡¶®‡¶ø‡¶π‡¶§ ‡ßß ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ ‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ‡ßü ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶¨‡ßã‡¶Æ‡¶æ ‡¶¨‡¶ø‡¶∏‡ßç‡¶´‡ßã‡¶∞‡¶£‡ßá ‡¶Ü‡¶ü ‡¶¨‡¶õ‡¶∞‡ßá...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üîç FEATURE EXTRACTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Prepare data\n",
        "X_train_text = train_df['cleaned_text']\n",
        "y_train = train_df['Label_Binary']\n",
        "X_test_text = test_df['cleaned_text']\n",
        "y_test = test_df['Label_Binary']\n",
        "\n",
        "# Step 1: TF-IDF Features\n",
        "print(\"\\n[1/3] Extracting TF-IDF features...\")\n",
        "print(\"   Parameters:\")\n",
        "print(\"   - Max features: 12,000\")\n",
        "print(\"   - N-grams: 1-3 (unigrams, bigrams, trigrams)\")\n",
        "print(\"   - Min document frequency: 2\")\n",
        "print(\"   - Max document frequency: 85%\")\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=12000,\n",
        "    ngram_range=(1, 3),\n",
        "    min_df=2,\n",
        "    max_df=0.85,\n",
        "    sublinear_tf=True,\n",
        "    strip_accents=None,\n",
        "    lowercase=False\n",
        ")\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_text)\n",
        "X_test_tfidf = tfidf.transform(X_test_text)\n",
        "\n",
        "print(f\"   ‚úÖ TF-IDF shape: {X_train_tfidf.shape}\")\n",
        "print(f\"   ‚úÖ Vocabulary size: {len(tfidf.vocabulary_):,} words\")\n",
        "\n",
        "# Step 2: Statistical Features\n",
        "print(\"\\n[2/3] Extracting statistical features...\")\n",
        "train_stats = extract_statistical_features(X_train_text)\n",
        "test_stats = extract_statistical_features(X_test_text)\n",
        "\n",
        "print(f\"   ‚úÖ Statistical features shape: {train_stats.shape}\")\n",
        "print(f\"   ‚úÖ Features: 9 (word count, char count, punctuation, etc.)\")\n",
        "\n",
        "# Step 3: Combine Features\n",
        "print(\"\\n[3/3] Combining TF-IDF + Statistical features...\")\n",
        "X_train_combined = hstack([X_train_tfidf, train_stats])\n",
        "X_test_combined = hstack([X_test_tfidf, test_stats])\n",
        "\n",
        "print(f\"   ‚úÖ Final combined features: {X_train_combined.shape}\")\n",
        "print(f\"   ‚úÖ Total features: {X_train_combined.shape[1]:,}\")\n",
        "\n",
        "print(\"\\n‚úÖ Feature extraction complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o7wAW1smTuF",
        "outputId": "af35413c-fcaa-4f31-c87f-5da4e429a9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üîç FEATURE EXTRACTION\n",
            "================================================================================\n",
            "\n",
            "[1/3] Extracting TF-IDF features...\n",
            "   Parameters:\n",
            "   - Max features: 12,000\n",
            "   - N-grams: 1-3 (unigrams, bigrams, trigrams)\n",
            "   - Min document frequency: 2\n",
            "   - Max document frequency: 85%\n",
            "   ‚úÖ TF-IDF shape: (42375, 12000)\n",
            "   ‚úÖ Vocabulary size: 12,000 words\n",
            "\n",
            "[2/3] Extracting statistical features...\n",
            "   ‚úÖ Statistical features shape: (42375, 9)\n",
            "   ‚úÖ Features: 9 (word count, char count, punctuation, etc.)\n",
            "\n",
            "[3/3] Combining TF-IDF + Statistical features...\n",
            "   ‚úÖ Final combined features: (42375, 12009)\n",
            "   ‚úÖ Total features: 12,009\n",
            "\n",
            "‚úÖ Feature extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ü§ñ TRAINING MACHINE LEARNING MODELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "models = {}\n",
        "\n",
        "# Model 1: Naive Bayes\n",
        "print(\"\\n[Model 1/5] Naive Bayes...\")\n",
        "start = time.time()\n",
        "nb_model = MultinomialNB(alpha=0.5)\n",
        "nb_model.fit(X_train_combined, y_train)\n",
        "nb_pred = nb_model.predict(X_test_combined)\n",
        "nb_acc = accuracy_score(y_test, nb_pred)\n",
        "models['Naive Bayes'] = (nb_model, nb_acc, nb_pred)\n",
        "print(f\"   ‚úÖ Accuracy: {nb_acc*100:.2f}% (Time: {time.time()-start:.1f}s)\")\n",
        "\n",
        "# Model 2: Logistic Regression\n",
        "print(\"\\n[Model 2/5] Logistic Regression...\")\n",
        "start = time.time()\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=3000,\n",
        "    C=2.0,\n",
        "    random_state=42,\n",
        "    solver='saga',\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ")\n",
        "lr_model.fit(X_train_combined, y_train)\n",
        "lr_pred = lr_model.predict(X_test_combined)\n",
        "lr_acc = accuracy_score(y_test, lr_pred)\n",
        "models['Logistic Regression'] = (lr_model, lr_acc, lr_pred)\n",
        "print(f\"   ‚úÖ Accuracy: {lr_acc*100:.2f}% (Time: {time.time()-start:.1f}s)\")\n",
        "\n",
        "# Model 3: Linear SVM (FAST)\n",
        "print(\"\\n[Model 3/5] Linear SVM (LinearSVC)...\")\n",
        "start = time.time()\n",
        "svm_model = LinearSVC(\n",
        "    C=1.5,\n",
        "    class_weight='balanced',\n",
        "    max_iter=2000,\n",
        "    random_state=42\n",
        ")\n",
        "svm_model.fit(X_train_combined, y_train)\n",
        "svm_pred = svm_model.predict(X_test_combined)\n",
        "svm_acc = accuracy_score(y_test, svm_pred)\n",
        "models['Linear SVM'] = (svm_model, svm_acc, svm_pred)\n",
        "print(f\"   ‚úÖ Accuracy: {svm_acc*100:.2f}% (Time: {time.time()-start:.1f}s)\")\n",
        "\n",
        "# Model 4: Random Forest\n",
        "print(\"\\n[Model 4/5] Random Forest...\")\n",
        "start = time.time()\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=60,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "rf_model.fit(X_train_combined, y_train)\n",
        "rf_pred = rf_model.predict(X_test_combined)\n",
        "rf_acc = accuracy_score(y_test, rf_pred)\n",
        "models['Random Forest'] = (rf_model, rf_acc, rf_pred)\n",
        "print(f\"   ‚úÖ Accuracy: {rf_acc*100:.2f}% (Time: {time.time()-start:.1f}s)\")\n",
        "\n",
        "# Model 5: Ensemble (Hard Voting)\n",
        "print(\"\\n[Model 5/5] Ensemble (Hard Voting)...\")\n",
        "print(\"   Combining: Logistic Regression + Linear SVM + Random Forest\")\n",
        "start = time.time()\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', lr_model),\n",
        "        ('svm', svm_model),\n",
        "        ('rf', rf_model)\n",
        "    ],\n",
        "    voting='hard',\n",
        "    n_jobs=-1\n",
        ")\n",
        "ensemble_model.fit(X_train_combined, y_train)\n",
        "ensemble_pred = ensemble_model.predict(X_test_combined)\n",
        "ensemble_acc = accuracy_score(y_test, ensemble_pred)\n",
        "models['Ensemble'] = (ensemble_model, ensemble_acc, ensemble_pred)\n",
        "print(f\"   ‚úÖ Accuracy: {ensemble_acc*100:.2f}% (Time: {time.time()-start:.1f}s)\")\n",
        "\n",
        "# Display all results\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä MODEL COMPARISON (sorted by accuracy)\")\n",
        "print(\"=\" * 80)\n",
        "for name, (_, acc, _) in sorted(models.items(), key=lambda x: x[1][1], reverse=True):\n",
        "    print(f\"   {name:25s}: {acc*100:.2f}%\")\n",
        "\n",
        "print(\"\\n‚úÖ All models trained successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrnO0iMLmVyc",
        "outputId": "1ed1981d-596c-4aea-ed98-2f7684d162d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ü§ñ TRAINING MACHINE LEARNING MODELS\n",
            "================================================================================\n",
            "\n",
            "[Model 1/5] Naive Bayes...\n",
            "   ‚úÖ Accuracy: 92.22% (Time: 0.4s)\n",
            "\n",
            "[Model 2/5] Logistic Regression...\n",
            "   ‚úÖ Accuracy: 68.06% (Time: 366.4s)\n",
            "\n",
            "[Model 3/5] Linear SVM (LinearSVC)...\n",
            "   ‚úÖ Accuracy: 90.08% (Time: 20.3s)\n",
            "\n",
            "[Model 4/5] Random Forest...\n",
            "   ‚úÖ Accuracy: 94.03% (Time: 153.2s)\n",
            "\n",
            "[Model 5/5] Ensemble (Hard Voting)...\n",
            "   Combining: Logistic Regression + Linear SVM + Random Forest\n",
            "   ‚úÖ Accuracy: 92.04% (Time: 530.2s)\n",
            "\n",
            "================================================================================\n",
            "üìä MODEL COMPARISON (sorted by accuracy)\n",
            "================================================================================\n",
            "   Random Forest            : 94.03%\n",
            "   Naive Bayes              : 92.22%\n",
            "   Ensemble                 : 92.04%\n",
            "   Linear SVM               : 90.08%\n",
            "   Logistic Regression      : 68.06%\n",
            "\n",
            "‚úÖ All models trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üèÜ SELECTING BEST MODEL & EVALUATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select best model\n",
        "best_model_name = max(models, key=lambda k: models[k][1])\n",
        "best_model, best_accuracy, best_predictions = models[best_model_name]\n",
        "\n",
        "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
        "print(f\"üèÜ Test Accuracy: {best_accuracy*100:.2f}%\")\n",
        "\n",
        "# Cross-validation\n",
        "print(f\"\\n‚è≥ Running 5-fold Cross-Validation...\")\n",
        "print(f\"   (This validates model stability across different data splits)\")\n",
        "cv_scores = cross_val_score(\n",
        "    best_model,\n",
        "    X_train_combined,\n",
        "    y_train,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Cross-Validation Results:\")\n",
        "print(f\"   Fold 1: {cv_scores[0]*100:.2f}%\")\n",
        "print(f\"   Fold 2: {cv_scores[1]*100:.2f}%\")\n",
        "print(f\"   Fold 3: {cv_scores[2]*100:.2f}%\")\n",
        "print(f\"   Fold 4: {cv_scores[3]*100:.2f}%\")\n",
        "print(f\"   Fold 5: {cv_scores[4]*100:.2f}%\")\n",
        "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "print(f\"   Mean:   {cv_scores.mean()*100:.2f}%\")\n",
        "print(f\"   Std:    ¬±{cv_scores.std()*100:.2f}%\")\n",
        "print(f\"   Range:  {cv_scores.min()*100:.2f}% - {cv_scores.max()*100:.2f}%\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"üìà DETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 80)\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    best_predictions,\n",
        "    labels=[0, 1],\n",
        "    target_names=['Fake News', 'Real News'],\n",
        "    digits=4\n",
        "))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä CONFUSION MATRIX\")\n",
        "print(\"=\" * 80)\n",
        "cm = confusion_matrix(y_test, best_predictions, labels=[0, 1])\n",
        "print(f\"\\n                    Predicted\")\n",
        "print(f\"                 Fake      Real\")\n",
        "print(f\"   Actual Fake   {cm[0][0]:6d}    {cm[0][1]:6d}\")\n",
        "print(f\"          Real   {cm[1][0]:6d}    {cm[1][1]:6d}\")\n",
        "\n",
        "# Calculate detailed metrics\n",
        "tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
        "\n",
        "fake_precision = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "fake_recall = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "fake_f1 = 2 * (fake_precision * fake_recall) / (fake_precision + fake_recall) if (fake_precision + fake_recall) > 0 else 0\n",
        "\n",
        "real_precision = tp / (fp + tp) if (fp + tp) > 0 else 0\n",
        "real_recall = tp / (fn + tp) if (fn + tp) > 0 else 0\n",
        "real_f1 = 2 * (real_precision * real_recall) / (real_precision + real_recall) if (real_precision + real_recall) > 0 else 0\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä KEY PERFORMANCE METRICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nüö® FAKE NEWS DETECTION:\")\n",
        "print(f\"   Precision: {fake_precision*100:.2f}% (How many detected fakes are actually fake)\")\n",
        "print(f\"   Recall:    {fake_recall*100:.2f}% (How many actual fakes we detected)\")\n",
        "print(f\"   F1-Score:  {fake_f1*100:.2f}%\")\n",
        "\n",
        "print(f\"\\n‚úÖ REAL NEWS DETECTION:\")\n",
        "print(f\"   Precision: {real_precision*100:.2f}% (How many detected reals are actually real)\")\n",
        "print(f\"   Recall:    {real_recall*100:.2f}% (How many actual reals we detected)\")\n",
        "print(f\"   F1-Score:  {real_f1*100:.2f}%\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è ERROR RATES:\")\n",
        "print(f\"   False Positive: {fp/(tn+fp)*100:.2f}% (Real news wrongly marked as fake)\")\n",
        "print(f\"   False Negative: {fn/(fn+tp)*100:.2f}% (Fake news wrongly marked as real)\")\n",
        "\n",
        "print(\"\\n‚úÖ Evaluation complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnlLTP1WmYTu",
        "outputId": "13727612-b91d-4cfd-df78-ad8f1c5d4705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üèÜ SELECTING BEST MODEL & EVALUATION\n",
            "================================================================================\n",
            "\n",
            "üèÜ Best Model: Random Forest\n",
            "üèÜ Test Accuracy: 94.03%\n",
            "\n",
            "‚è≥ Running 5-fold Cross-Validation...\n",
            "   (This validates model stability across different data splits)\n",
            "\n",
            "‚úÖ Cross-Validation Results:\n",
            "   Fold 1: 94.55%\n",
            "   Fold 2: 94.31%\n",
            "   Fold 3: 94.11%\n",
            "   Fold 4: 94.23%\n",
            "   Fold 5: 94.06%\n",
            "   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "   Mean:   94.25%\n",
            "   Std:    ¬±0.17%\n",
            "   Range:  94.06% - 94.55%\n",
            "\n",
            "================================================================================\n",
            "üìà DETAILED CLASSIFICATION REPORT\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Fake News     0.9563    0.9822    0.9690      8636\n",
            "   Real News     0.2667    0.1261    0.1713       444\n",
            "\n",
            "    accuracy                         0.9403      9080\n",
            "   macro avg     0.6115    0.5541    0.5701      9080\n",
            "weighted avg     0.9225    0.9403    0.9300      9080\n",
            "\n",
            "================================================================================\n",
            "üìä CONFUSION MATRIX\n",
            "================================================================================\n",
            "\n",
            "                    Predicted\n",
            "                 Fake      Real\n",
            "   Actual Fake     8482       154\n",
            "          Real      388        56\n",
            "\n",
            "================================================================================\n",
            "üìä KEY PERFORMANCE METRICS\n",
            "================================================================================\n",
            "\n",
            "üö® FAKE NEWS DETECTION:\n",
            "   Precision: 95.63% (How many detected fakes are actually fake)\n",
            "   Recall:    98.22% (How many actual fakes we detected)\n",
            "   F1-Score:  96.90%\n",
            "\n",
            "‚úÖ REAL NEWS DETECTION:\n",
            "   Precision: 26.67% (How many detected reals are actually real)\n",
            "   Recall:    12.61% (How many actual reals we detected)\n",
            "   F1-Score:  17.13%\n",
            "\n",
            "‚ö†Ô∏è ERROR RATES:\n",
            "   False Positive: 1.78% (Real news wrongly marked as fake)\n",
            "   False Negative: 87.39% (Fake news wrongly marked as real)\n",
            "\n",
            "‚úÖ Evaluation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "47l0sI3xmcFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"üíæ SAVING TRAINED MODELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save best model\n",
        "print(\"\\n[1/4] Saving best model...\")\n",
        "with open('fake_news_model.pkl', 'wb') as f:\n",
        "    pickle.dump(best_model, f)\n",
        "print(\"   ‚úÖ fake_news_model.pkl\")\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "print(\"\\n[2/4] Saving TF-IDF vectorizer...\")\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf, f)\n",
        "print(\"   ‚úÖ tfidf_vectorizer.pkl\")\n",
        "\n",
        "# Save feature configuration\n",
        "print(\"\\n[3/4] Saving feature configuration...\")\n",
        "feature_config = {\n",
        "    'extract_stats': extract_statistical_features,\n",
        "    'clean_text': clean_bangla_text,\n",
        "    'remove_stopwords': remove_stopwords,\n",
        "    'stopwords': bangla_stopwords\n",
        "}\n",
        "with open('feature_config.pkl', 'wb') as f:\n",
        "    pickle.dump(feature_config, f)\n",
        "print(\"   ‚úÖ feature_config.pkl\")\n",
        "\n",
        "# Save comprehensive metadata\n",
        "print(\"\\n[4/4] Saving model metadata...\")\n",
        "metadata = {\n",
        "    'model_name': best_model_name,\n",
        "    'accuracy': float(best_accuracy),\n",
        "    'cv_mean': float(cv_scores.mean()),\n",
        "    'cv_std': float(cv_scores.std()),\n",
        "    'training_samples': len(train_df),\n",
        "    'test_samples': len(test_df),\n",
        "    'vocabulary_size': len(tfidf.vocabulary_),\n",
        "    'fake_precision': float(fake_precision),\n",
        "    'fake_recall': float(fake_recall),\n",
        "    'fake_f1': float(fake_f1),\n",
        "    'real_precision': float(real_precision),\n",
        "    'real_recall': float(real_recall),\n",
        "    'real_f1': float(real_f1),\n",
        "    'false_positive_rate': float(fp/(tn+fp)),\n",
        "    'false_negative_rate': float(fn/(fn+tp)),\n",
        "    'ngram_range': '1-3',\n",
        "    'max_features': 12000,\n",
        "    'has_statistical_features': True\n",
        "}\n",
        "with open('model_metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "print(\"   ‚úÖ model_metadata.pkl\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ ALL MODEL FILES SAVED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# List saved files with sizes\n",
        "print(\"\\nüìÅ Saved Files:\")\n",
        "import os\n",
        "for filename in ['fake_news_model.pkl', 'tfidf_vectorizer.pkl', 'feature_config.pkl', 'model_metadata.pkl']:\n",
        "    if os.path.exists(filename):\n",
        "        size = os.path.getsize(filename) / (1024 * 1024)  # MB\n",
        "        print(f\"   ‚úÖ {filename:25s} - {size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r8sUYotmanE",
        "outputId": "cc20536f-16ea-47c9-d345-cd9de5111fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "üíæ SAVING TRAINED MODELS\n",
            "================================================================================\n",
            "\n",
            "[1/4] Saving best model...\n",
            "   ‚úÖ fake_news_model.pkl\n",
            "\n",
            "[2/4] Saving TF-IDF vectorizer...\n",
            "   ‚úÖ tfidf_vectorizer.pkl\n",
            "\n",
            "[3/4] Saving feature configuration...\n",
            "   ‚úÖ feature_config.pkl\n",
            "\n",
            "[4/4] Saving model metadata...\n",
            "   ‚úÖ model_metadata.pkl\n",
            "\n",
            "================================================================================\n",
            "‚úÖ ALL MODEL FILES SAVED SUCCESSFULLY!\n",
            "================================================================================\n",
            "\n",
            "üìÅ Saved Files:\n",
            "   ‚úÖ fake_news_model.pkl       - 73.95 MB\n",
            "   ‚úÖ tfidf_vectorizer.pkl      - 0.51 MB\n",
            "   ‚úÖ feature_config.pkl        - 0.00 MB\n",
            "   ‚úÖ model_metadata.pkl        - 0.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"‚¨áÔ∏è DOWNLOAD TRAINED MODELS TO YOUR COMPUTER\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüì¶ Preparing to download 4 model files...\")\n",
        "print(\"   These will be saved to your Downloads folder\")\n",
        "print(\"\\n‚è≥ Downloading (this may take 30-60 seconds)...\\n\")\n",
        "\n",
        "# Download all model files\n",
        "try:\n",
        "    files.download('fake_news_model.pkl')\n",
        "    print(\"   ‚úÖ fake_news_model.pkl downloaded\")\n",
        "\n",
        "    files.download('tfidf_vectorizer.pkl')\n",
        "    print(\"   ‚úÖ tfidf_vectorizer.pkl downloaded\")\n",
        "\n",
        "    files.download('feature_config.pkl')\n",
        "    print(\"   ‚úÖ feature_config.pkl downloaded\")\n",
        "\n",
        "    files.download('model_metadata.pkl')\n",
        "    print(\"   ‚úÖ model_metadata.pkl downloaded\")\n",
        "\n",
        "    print(\"\\n‚úÖ ALL FILES DOWNLOADED SUCCESSFULLY!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è Download error: {e}\")\n",
        "    print(\"Alternative: Click folder icon on left ‚Üí Right-click each .pkl file ‚Üí Download\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìã NEXT STEPS:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"1. Check your Downloads folder for 4 .pkl files\")\n",
        "print(\"2. Copy all 4 files to your project's 'models/' folder\")\n",
        "print(\"3. Update your app.py with the new version\")\n",
        "print(\"4. Run: python app.py\")\n",
        "print(\"5. Open: http://localhost:5000\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "nUtzKqA6meD-",
        "outputId": "2125fc08-2f5e-4059-82a7-5bf7ca1c669d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "‚¨áÔ∏è DOWNLOAD TRAINED MODELS TO YOUR COMPUTER\n",
            "================================================================================\n",
            "\n",
            "üì¶ Preparing to download 4 model files...\n",
            "   These will be saved to your Downloads folder\n",
            "\n",
            "‚è≥ Downloading (this may take 30-60 seconds)...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_46041235-36e6-41d6-90cf-ae170dda95c3\", \"fake_news_model.pkl\", 77537383)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úÖ fake_news_model.pkl downloaded\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c0790518-549e-4f57-bad2-66efcf51e2d0\", \"tfidf_vectorizer.pkl\", 530208)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úÖ tfidf_vectorizer.pkl downloaded\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_656b5a98-bddb-4eb4-8efc-942c40a85e26\", \"feature_config.pkl\", 205)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úÖ feature_config.pkl downloaded\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1fd6bd16-0cb0-4a6b-b81a-074f39d92993\", \"model_metadata.pkl\", 427)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ‚úÖ model_metadata.pkl downloaded\n",
            "\n",
            "‚úÖ ALL FILES DOWNLOADED SUCCESSFULLY!\n",
            "\n",
            "================================================================================\n",
            "üìã NEXT STEPS:\n",
            "================================================================================\n",
            "1. Check your Downloads folder for 4 .pkl files\n",
            "2. Copy all 4 files to your project's 'models/' folder\n",
            "3. Update your app.py with the new version\n",
            "4. Run: python app.py\n",
            "5. Open: http://localhost:5000\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üéâ TRAINING COMPLETE - FINAL SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nüìä MODEL PERFORMANCE:\")\n",
        "print(f\"   Model Type:     {best_model_name}\")\n",
        "print(f\"   Test Accuracy:  {best_accuracy*100:.2f}%\")\n",
        "print(f\"   CV Accuracy:    {cv_scores.mean()*100:.2f}% (¬±{cv_scores.std()*100:.2f}%)\")\n",
        "print(f\"   Fake Detection: {fake_recall*100:.2f}%\")\n",
        "print(f\"   Real Detection: {real_recall*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nüìà TRAINING DATA:\")\n",
        "print(f\"   Training Samples: {len(train_df):,}\")\n",
        "print(f\"   Test Samples:     {len(test_df):,}\")\n",
        "print(f\"   Vocabulary:       {len(tfidf.vocabulary_):,} words\")\n",
        "print(f\"   Features:         TF-IDF (12K) + Statistical (9)\")\n",
        "\n",
        "print(f\"\\nüéØ MODEL CHARACTERISTICS:\")\n",
        "print(f\"   ‚úÖ Enhanced preprocessing (minimal stopwords)\")\n",
        "print(f\"   ‚úÖ TF-IDF with trigrams (1-3 n-grams)\")\n",
        "print(f\"   ‚úÖ Statistical features included\")\n",
        "print(f\"   ‚úÖ {'Ensemble voting' if 'Ensemble' in best_model_name else 'Single model'}\")\n",
        "print(f\"   ‚úÖ Cross-validated for stability\")\n",
        "print(f\"   ‚úÖ Balanced class weights\")\n",
        "\n",
        "print(f\"\\nüíæ SAVED FILES:\")\n",
        "print(f\"   ‚úÖ fake_news_model.pkl\")\n",
        "print(f\"   ‚úÖ tfidf_vectorizer.pkl\")\n",
        "print(f\"   ‚úÖ feature_config.pkl\")\n",
        "print(f\"   ‚úÖ model_metadata.pkl\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üß™ QUICK TEST - Let's predict a sample!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Test with a sample\n",
        "test_sample = test_df['cleaned_text'].iloc[0]\n",
        "test_label = y_test.iloc[0]\n",
        "\n",
        "# Extract features\n",
        "test_tfidf = tfidf.transform([test_sample])\n",
        "test_stats = extract_statistical_features([test_sample])\n",
        "test_features = hstack([test_tfidf, test_stats])\n",
        "\n",
        "# Predict\n",
        "prediction = best_model.predict(test_features)[0]\n",
        "proba = best_model.predict_proba(test_features)[0]\n",
        "\n",
        "print(f\"\\nüì∞ Sample News (first 150 chars):\")\n",
        "print(f\"   {test_df['full_text'].iloc[0][:150]}...\")\n",
        "print(f\"\\nüîç Prediction:\")\n",
        "print(f\"   Predicted: {'Real News' if prediction == 1 else 'Fake News'}\")\n",
        "print(f\"   Actual:    {'Real News' if test_label == 1 else 'Fake News'}\")\n",
        "print(f\"   Correct:   {'‚úÖ YES' if prediction == test_label else '‚ùå NO'}\")\n",
        "print(f\"   Confidence: {max(proba)*100:.2f}%\")\n",
        "print(f\"   Probabilities: Fake={proba[0]*100:.2f}%, Real={proba[1]*100:.2f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ ALL DONE! Your model is ready to use!\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tPFKLoxmgqv",
        "outputId": "bdae150b-3869-496d-b4dc-1f0d772d49f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "üéâ TRAINING COMPLETE - FINAL SUMMARY\n",
            "================================================================================\n",
            "\n",
            "üìä MODEL PERFORMANCE:\n",
            "   Model Type:     Random Forest\n",
            "   Test Accuracy:  94.03%\n",
            "   CV Accuracy:    94.25% (¬±0.17%)\n",
            "   Fake Detection: 98.22%\n",
            "   Real Detection: 12.61%\n",
            "\n",
            "üìà TRAINING DATA:\n",
            "   Training Samples: 42,375\n",
            "   Test Samples:     9,080\n",
            "   Vocabulary:       12,000 words\n",
            "   Features:         TF-IDF (12K) + Statistical (9)\n",
            "\n",
            "üéØ MODEL CHARACTERISTICS:\n",
            "   ‚úÖ Enhanced preprocessing (minimal stopwords)\n",
            "   ‚úÖ TF-IDF with trigrams (1-3 n-grams)\n",
            "   ‚úÖ Statistical features included\n",
            "   ‚úÖ Single model\n",
            "   ‚úÖ Cross-validated for stability\n",
            "   ‚úÖ Balanced class weights\n",
            "\n",
            "üíæ SAVED FILES:\n",
            "   ‚úÖ fake_news_model.pkl\n",
            "   ‚úÖ tfidf_vectorizer.pkl\n",
            "   ‚úÖ feature_config.pkl\n",
            "   ‚úÖ model_metadata.pkl\n",
            "\n",
            "================================================================================\n",
            "üß™ QUICK TEST - Let's predict a sample!\n",
            "================================================================================\n",
            "\n",
            "üì∞ Sample News (first 150 chars):\n",
            "   ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø‡¶§‡ßá ‡¶¨‡ßÄ‡¶Æ‡¶æ‡¶∞ ‡¶Ö‡¶¨‡¶¶‡¶æ‡¶® ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡ß¶.‡ßØ ‡¶∂‡¶§‡¶æ‡¶Ç‡¶∂ ‡¶Ü‡¶∞‡ßç‡¶•‡¶ø‡¶ï ‡¶ñ‡¶æ‡¶§‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶¨‡ßÄ‡¶Æ‡¶æ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶π‡¶≤‡ßá‡¶ì ‡¶Æ‡ßã‡¶ü ‡¶¶‡ßá‡¶∂‡¶ú ‡¶â‡ßé‡¶™‡¶æ‡¶¶‡¶®‡ßá (‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø) ‡¶¨‡ßÄ‡¶Æ‡¶æ‡¶∞ ‡¶Ö‡¶¨‡¶¶‡¶æ‡¶® ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡ß¶.‡ßØ ‡¶∂‡¶§‡¶æ‡¶Ç‡¶∂‡•§ ‡¶§‡¶¨‡ßá ‡¶¨‡ßÄ‡¶Æ‡¶æ ‡¶â‡¶®‡ßç‡¶®...\n",
            "\n",
            "üîç Prediction:\n",
            "   Predicted: Fake News\n",
            "   Actual:    Fake News\n",
            "   Correct:   ‚úÖ YES\n",
            "   Confidence: 96.25%\n",
            "   Probabilities: Fake=96.25%, Real=3.75%\n",
            "\n",
            "================================================================================\n",
            "‚úÖ ALL DONE! Your model is ready to use!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IMkxrkS1mi0g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}